version: '3.8'

services:
  # Redis for rate limiting and caching
  redis:
    image: redis:7-alpine
    container_name: scraper-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - scraper-network
    restart: unless-stopped

  # Main Search Scraping API
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraper-api
    ports:
      - "8000:8000"
    environment:
      # API Configuration
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_WORKERS=4
      - DEBUG=false
      
      # Rate Limiting (50+ requests per minute)
      - MAX_SEARCH_REQUESTS_PER_MINUTE=100
      - MAX_WEBSITE_REQUESTS_PER_MINUTE=60
      - MAX_CONCURRENT_REQUESTS=100
      
      # Redis
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      
      # Proxy Configuration
      - USE_PROXY=true
      - PROXY_ROTATION=true
      - PROXY_TIMEOUT=15
      - AUTO_FETCH_PROXIES=true
      
      # Request Configuration (fast)
      - REQUEST_TIMEOUT=15
      - MAX_RETRIES=2
      - RETRY_DELAY=0.5
      
      # Search Configuration
      - DEFAULT_SEARCH_ENGINE=google
      - ENABLE_FALLBACK=true
      - CACHE_RESULTS=true
      - CACHE_TTL=300
      
      # Captcha Solving (optional - add your API keys)
      - ENABLE_CAPTCHA_SOLVER=true
      - TWOCAPTCHA_API_KEY=${TWOCAPTCHA_API_KEY:-}
      - ANTICAPTCHA_API_KEY=${ANTICAPTCHA_API_KEY:-}
      - CAPMONSTER_API_KEY=${CAPMONSTER_API_KEY:-}
      
      # Scraping
      - JAVASCRIPT_RENDERING=true
      - BROWSER_HEADLESS=true
      - PAGE_LOAD_TIMEOUT=15
      
      # Logging
      - LOG_LEVEL=INFO
      - LOG_FILE=logs/scraper.log
    
    volumes:
      - ./config:/app/config
      - ./logs:/app/logs
    
    depends_on:
      redis:
        condition: service_healthy
    
    networks:
      - scraper-network
    
    restart: unless-stopped
    
    # Resource limits for high performance
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G

  # Development version with hot reload
  api-dev:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraper-api-dev
    ports:
      - "8001:8000"
    environment:
      - DEBUG=true
      - LOG_LEVEL=DEBUG
      - USE_PROXY=false
      - REDIS_HOST=redis
    volumes:
      - ./app:/app/app
      - ./config:/app/config
      - ./logs:/app/logs
    command: ["python", "-m", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    depends_on:
      - redis
    networks:
      - scraper-network
    profiles:
      - dev

  # Nginx reverse proxy (production)
  nginx:
    image: nginx:alpine
    container_name: scraper-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - api
    networks:
      - scraper-network
    restart: unless-stopped
    profiles:
      - production

volumes:
  redis-data:
    driver: local

networks:
  scraper-network:
    driver: bridge
