# ğŸ” Advanced Search Engine Scraping System

A high-performance, production-ready search engine scraping system with multi-engine support, automatic fallback, and advanced anti-detection capabilities.

## âœ¨ Features

### ğŸ” Multi-Engine Support
- **Google** - Multiple fallback strategies (direct, mobile, browser, library)
- **DuckDuckGo** - Uses native library (fastest, most reliable)
- **Bing** - Full anti-detection support
- **Yahoo** - Complete search support

### ğŸš€ High Performance
- **50+ requests/minute** throughput
- **< 2 second** response time (average)
- Concurrent batch processing
- Connection pooling
- Async/await architecture

### ğŸ•µï¸ Anti-Detection Strategies (6+)
1. **Fingerprint Randomization** - Browser fingerprints rotate automatically
2. **User-Agent Rotation** - Realistic browser signatures
3. **Proxy Rotation** - Auto-fetch and rotate proxies
4. **TLS Fingerprint Variation** - Randomized SSL/TLS settings
5. **Request Header Randomization** - Sec-CH-UA, Accept headers, etc.
6. **Stealth Browser Mode** - Playwright/Selenium with anti-detection scripts

### ğŸŒ Proxy Management
- **Auto-fetch free proxies** from 5+ public sources
- Support for HTTP, HTTPS, SOCKS4, SOCKS5
- Automatic health checking and rotation
- Custom proxy support via environment variables

### ğŸ¤– Captcha Handling
- reCAPTCHA v2 (audio challenge)
- reCAPTCHA v3 (behavior simulation)
- Cloudflare Turnstile bypass
- Image captcha OCR (Tesseract + EasyOCR)
- hCaptcha support

### ğŸ”„ Automatic Fallback
When one search engine fails, automatically tries the next:
```
Google â†’ DuckDuckGo â†’ Bing â†’ Yahoo
```

### ğŸ“Š Search Types
- Web search
- News search
- Image search
- Video search

## ğŸš€ Quick Start

### Using Docker (Recommended)

```bash
# Clone the repository
git clone <repository-url>
cd search-scraper

# Copy environment file
cp .env.example .env

# Start the services
docker-compose up -d

# Check health
curl http://localhost:8000/health
```

### Local Development

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium

# Run the server
python -m uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ“– API Usage

### Unified Search (Recommended)
The unified endpoint automatically handles fallback between search engines:

```bash
curl -X POST http://localhost:8000/api/v1/search/unified \
  -H "Content-Type: application/json" \
  -d '{
    "query": "python web scraping",
    "num_results": 10,
    "search_type": "all",
    "enable_fallback": true
  }'
```

### Google Search
```bash
curl -X POST http://localhost:8000/api/v1/search/google \
  -H "Content-Type: application/json" \
  -d '{
    "query": "machine learning",
    "num_results": 10,
    "search_type": "all"
  }'
```

### DuckDuckGo Search (Fastest)
```bash
curl -X POST http://localhost:8000/api/v1/search/duckduckgo \
  -H "Content-Type: application/json" \
  -d '{
    "query": "artificial intelligence",
    "num_results": 10
  }'
```

### Bing Search
```bash
curl -X POST http://localhost:8000/api/v1/search/bing \
  -H "Content-Type: application/json" \
  -d '{
    "query": "data science",
    "num_results": 10
  }'
```

### Yahoo Search
```bash
curl -X POST http://localhost:8000/api/v1/search/yahoo \
  -H "Content-Type: application/json" \
  -d '{
    "query": "web development",
    "num_results": 10
  }'
```

### Batch Search (Multiple Queries)
```bash
curl -X POST http://localhost:8000/api/v1/search/batch \
  -H "Content-Type: application/json" \
  -d '{
    "queries": ["python", "javascript", "rust", "go"],
    "num_results": 10,
    "engine": "auto"
  }'
```

### Search All Engines Concurrently
```bash
curl -X POST http://localhost:8000/api/v1/search/all-engines \
  -H "Content-Type: application/json" \
  -d '{
    "query": "cloud computing",
    "num_results": 10
  }'
```

### News Search
```bash
curl -X POST http://localhost:8000/api/v1/search/unified \
  -H "Content-Type: application/json" \
  -d '{
    "query": "technology news",
    "search_type": "news",
    "num_results": 20
  }'
```

### Image Search
```bash
curl -X POST http://localhost:8000/api/v1/search/unified \
  -H "Content-Type: application/json" \
  -d '{
    "query": "nature wallpaper",
    "search_type": "images",
    "num_results": 20
  }'
```

### DuckDuckGo Instant Answer
```bash
curl http://localhost:8000/api/v1/search/instant/python%20programming
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `API_HOST` | `0.0.0.0` | API host address |
| `API_PORT` | `8000` | API port |
| `API_WORKERS` | `4` | Number of worker processes |
| `DEBUG` | `False` | Enable debug mode |
| `MAX_SEARCH_REQUESTS_PER_MINUTE` | `120` | Rate limit for searches |
| `MAX_CONCURRENT_REQUESTS` | `100` | Max concurrent connections |
| `USE_PROXY` | `True` | Enable proxy rotation |
| `AUTO_FETCH_PROXIES` | `True` | Auto-fetch free proxies |
| `CUSTOM_PROXIES` | `` | Comma-separated custom proxies |
| `ENABLE_CAPTCHA_SOLVER` | `True` | Enable captcha solving |
| `ENABLE_FALLBACK` | `True` | Enable search engine fallback |
| `FALLBACK_ORDER` | `google,duckduckgo,bing,yahoo` | Fallback order |
| `REQUEST_TIMEOUT` | `15` | Request timeout in seconds |
| `PAGE_LOAD_TIMEOUT` | `15` | Browser page load timeout |

### Custom Proxies

Add your own proxies via environment variable:

```bash
CUSTOM_PROXIES=http://proxy1:8080,socks5://user:pass@proxy2:1080,http://proxy3:3128
```

Supported formats:
- `http://host:port`
- `https://host:port`
- `socks4://host:port`
- `socks5://host:port`
- `socks5://user:pass@host:port`

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI Application                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Unified Search Engine                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Google  â”‚ â”‚ DuckDuckGo  â”‚ â”‚ Bing â”‚ â”‚ Yahoo â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜            â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                         â–¼                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚              â”‚   Request Handler   â”‚                         â”‚
â”‚              â”‚  (6+ strategies)    â”‚                         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                         â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  Anti-Detection Layer                â”‚    â”‚
â”‚  â”‚  â€¢ Fingerprint Randomization  â€¢ TLS Variation       â”‚    â”‚
â”‚  â”‚  â€¢ User-Agent Rotation        â€¢ Header Randomization â”‚    â”‚
â”‚  â”‚  â€¢ Proxy Rotation             â€¢ Stealth Browser      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Proxy Manager  â”‚  â”‚ Captcha Solver â”‚  â”‚ Rate Limiter â”‚   â”‚
â”‚  â”‚ (auto-fetch)   â”‚  â”‚ (multi-type)   â”‚  â”‚ (token bucket)â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ search_scraper.py   # Search API endpoints
â”‚   â”‚   â””â”€â”€ website_scraper.py  # Website scraping endpoints
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py         # Configuration management
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ captcha_solver.py   # Captcha detection & solving
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py    # Proxy fetching & rotation
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py     # Rate limiting
â”‚   â”‚   â””â”€â”€ request_handler.py  # Request handling with anti-detection
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ google_scraper.py   # Google search scraper
â”‚   â”‚   â”œâ”€â”€ duckduckgo_scraper.py # DuckDuckGo scraper
â”‚   â”‚   â”œâ”€â”€ bing_scraper.py     # Bing search scraper
â”‚   â”‚   â”œâ”€â”€ yahoo_scraper.py    # Yahoo search scraper
â”‚   â”‚   â””â”€â”€ generic_scraper.py  # Generic website scraper
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ contact_extractor.py
â”‚   â”‚   â””â”€â”€ content_parser.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ helpers.py
â”‚       â””â”€â”€ user_agents.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ proxies.txt             # HTTP proxies (optional)
â”‚   â””â”€â”€ socks_proxies.txt       # SOCKS proxies (optional)
â”œâ”€â”€ logs/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ”§ Advanced Usage

### Python SDK

```python
import asyncio
from app.scrapers import GoogleScraper, DuckDuckGoScraper, BingScraper, YahooScraper

async def main():
    # Initialize scrapers
    google = GoogleScraper()
    ddg = DuckDuckGoScraper()
    bing = BingScraper()
    yahoo = YahooScraper()
    
    # Google search with fallback
    result = await google.search(
        query="python web scraping",
        num_results=10,
        search_type="all",
        fast_mode=True
    )
    
    if result['success']:
        for item in result['results']:
            print(f"{item['title']}: {item['url']}")
    
    # DuckDuckGo (fastest)
    result = await ddg.search(
        query="machine learning",
        num_results=20
    )
    
    # Batch search
    queries = ["python", "javascript", "rust"]
    tasks = [ddg.search(q, num_results=5) for q in queries]
    results = await asyncio.gather(*tasks)

asyncio.run(main())
```

### Custom Proxy Configuration

```python
from app.core.proxy_manager import proxy_manager

# Add custom proxies at runtime
proxy_manager._add_proxy("http://myproxy:8080", source="custom")
proxy_manager._add_proxy("socks5://user:pass@socks.example.com:1080", source="custom")
```

## ğŸ³ Docker Commands

```bash
# Build and start
docker-compose up -d --build

# View logs
docker-compose logs -f api

# Stop services
docker-compose down

# Start with development mode
docker-compose --profile dev up -d

# Start with nginx (production)
docker-compose --profile production up -d

# Rebuild without cache
docker-compose build --no-cache
```

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check |
| `/status` | GET | Detailed status |
| `/proxy-stats` | GET | Proxy statistics |
| `/api/v1/search/unified` | POST | Unified search with fallback |
| `/api/v1/search/all-engines` | POST | Search all engines concurrently |
| `/api/v1/search/google` | POST | Google search |
| `/api/v1/search/duckduckgo` | POST | DuckDuckGo search |
| `/api/v1/search/bing` | POST | Bing search |
| `/api/v1/search/yahoo` | POST | Yahoo search |
| `/api/v1/search/batch` | POST | Batch search |
| `/api/v1/search/instant/{query}` | GET | DuckDuckGo instant answer |
| `/docs` | GET | Swagger UI documentation |
| `/redoc` | GET | ReDoc documentation |

## ğŸ”’ Security Considerations

- Never expose the API directly to the internet without authentication
- Use a reverse proxy (nginx) with rate limiting in production
- Monitor proxy health and rotate frequently
- Be respectful of search engines' terms of service
- Implement proper error handling and logging

## ğŸ“ˆ Performance Tips

1. **Use DuckDuckGo for speed** - It uses a native library, no scraping needed
2. **Enable fast_mode** - Reduces pages fetched per search
3. **Use batch endpoint** - For multiple queries
4. **Configure proper timeouts** - Lower values = faster failures
5. **Monitor proxy health** - Remove slow/dead proxies
6. **Use Redis** - For distributed rate limiting

## ğŸ› Troubleshooting

### Common Issues

**1. Brotli encoding error**
```bash
pip install Brotli brotlicffi
```

**2. Playwright not working**
```bash
playwright install chromium
playwright install-deps chromium
```

**3. No results from Google**
- Google is heavily blocked; use unified search with fallback
- DuckDuckGo is most reliable

**4. Slow response times**
- Enable proxy rotation
- Use fast_mode=true
- Reduce num_results

**5. Captcha detected**
- System will attempt to solve automatically
- Try different search engine
- Use more proxies

## ğŸ“„ License

MIT License - See LICENSE file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please read the contributing guidelines first.

## ğŸ“ Support

For issues and feature requests, please use the GitHub issue tracker.
