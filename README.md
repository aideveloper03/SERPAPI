# High-Volume Web Scraping System

A production-ready, high-volume concurrent web scraping system with search engine and generic website scrapers. Built with Python, FastAPI, and advanced scraping techniques.

[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-green.svg)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

## ğŸš€ Features

### Core Capabilities

- **High-Volume Scraping**: 60+ search results/min, 30+ websites/min
- **Concurrent Processing**: Handle 50+ simultaneous requests
- **Multiple Fallback Methods**: 2-3 strategies per request for maximum success
- **Proxy Rotation**: Automatic IP rotation with health checking
- **Captcha Solving**: Automatic detection and solving (audio, image recognition)
- **Browser Automation**: Playwright and Selenium for JavaScript-heavy sites
- **Content Categorization**: Intelligent extraction of paragraphs, contacts, metadata
- **Rate Limiting**: Built-in token bucket rate limiting with Redis support
- **RESTful API**: Clean, documented FastAPI endpoints

### Search Engine Scraping

- **Google Search**: All, News, Images, Videos
- **DuckDuckGo Search**: All, News, Images, Videos
- **Batch Processing**: Scrape multiple queries concurrently
- **Combined Search**: Query both engines simultaneously

### Website Scraping

- **Generic Scraping**: Works with any website
- **Content Extraction**:
  - Title and metadata
  - Headings (h1-h6)
  - Paragraphs with context
  - Lists and tables
  - Images and links
  - Structured data (JSON-LD, Open Graph, microdata)
  
- **Contact Extraction**:
  - Email addresses (validated)
  - Phone numbers (international formats)
  - Social media links (Facebook, Twitter, LinkedIn, Instagram, YouTube)
  - Physical addresses
  
- **Advanced Features**:
  - Deep scraping (follow links)
  - Batch scraping (multiple URLs)
  - Quick extract endpoints (contacts, content, metadata only)

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Usage](#-usage)
- [API Documentation](#-api-documentation)
- [Docker Deployment](#-docker-deployment)
- [Performance](#-performance)
- [Architecture](#-architecture)
- [Contributing](#-contributing)
- [License](#-license)

## âš¡ Quick Start

### Using Docker (Recommended)

```bash
# Clone repository
git clone <repository-url>
cd web-scraping-system

# Start services
docker-compose up -d

# Check status
curl http://localhost:8000/health
```

### Manual Setup

```bash
# Clone repository
git clone <repository-url>
cd web-scraping-system

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers (optional)
playwright install chromium

# Configure
cp .env.example .env
# Edit .env with your settings

# Run application
python app/main.py
```

### First Request

```bash
# Search Google
curl -X POST http://localhost:8000/api/v1/search/google \
  -H "Content-Type: application/json" \
  -d '{"query": "python web scraping", "num_results": 5}'

# Scrape website
curl -X POST http://localhost:8000/api/v1/website/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.9 or higher
- pip (Python package manager)
- Redis (optional, for distributed rate limiting)
- Tesseract OCR (optional, for captcha solving)

### Step-by-Step Installation

1. **Clone the repository**

```bash
git clone <repository-url>
cd web-scraping-system
```

2. **Create virtual environment**

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows
```

3. **Install Python dependencies**

```bash
pip install -r requirements.txt
```

4. **Install optional components**

```bash
# Playwright (for JavaScript-heavy sites)
playwright install chromium

# Redis (Ubuntu/Debian)
sudo apt-get install redis-server

# Tesseract OCR (Ubuntu/Debian)
sudo apt-get install tesseract-ocr
```

5. **Configure the application**

```bash
cp .env.example .env
# Edit .env with your settings
```

6. **Run the application**

```bash
python app/main.py
```

See [docs/SETUP.md](docs/SETUP.md) for detailed installation instructions.

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file:

```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# Rate Limiting
MAX_SEARCH_REQUESTS_PER_MINUTE=60
MAX_WEBSITE_REQUESTS_PER_MINUTE=30
MAX_CONCURRENT_REQUESTS=50

# Redis (optional)
REDIS_HOST=localhost
REDIS_PORT=6379

# Proxy (optional)
USE_PROXY=True
PROXY_ROTATION=True

# Scraping
JAVASCRIPT_RENDERING=True
BROWSER_HEADLESS=True
REQUEST_TIMEOUT=30
MAX_RETRIES=3
```

### Proxy Configuration

Add proxies to `config/proxies.txt`:

```
http://proxy1.example.com:8080
http://user:pass@proxy2.example.com:3128
socks5://proxy3.example.com:1080
```

See [docs/CONFIGURATION.md](docs/CONFIGURATION.md) for detailed configuration options.

## ğŸ“– Usage

### Python Example

```python
import requests

BASE_URL = "http://localhost:8000"

# Google Search
response = requests.post(
    f"{BASE_URL}/api/v1/search/google",
    json={
        "query": "python web scraping",
        "search_type": "all",
        "num_results": 10
    }
)
results = response.json()
print(f"Found {results['total_results']} results")

# Scrape Website
response = requests.post(
    f"{BASE_URL}/api/v1/website/scrape",
    json={
        "url": "https://example.com",
        "extract_contacts": True
    }
)
data = response.json()
print(f"Title: {data['title']}")
print(f"Emails: {data['contacts']['emails']}")

# Batch Scrape
response = requests.post(
    f"{BASE_URL}/api/v1/website/scrape/batch",
    json={
        "urls": [
            "https://example.com",
            "https://example.org",
            "https://example.net"
        ],
        "max_concurrent": 10
    }
)
batch_data = response.json()
print(f"Scraped {batch_data['successful']}/{batch_data['total_urls']} sites")
```

### cURL Examples

```bash
# Google Search - All Results
curl -X POST http://localhost:8000/api/v1/search/google \
  -H "Content-Type: application/json" \
  -d '{"query": "machine learning", "num_results": 10}'

# Google Search - News
curl -X POST http://localhost:8000/api/v1/search/google \
  -H "Content-Type: application/json" \
  -d '{"query": "latest news", "search_type": "news", "num_results": 20}'

# DuckDuckGo Search
curl -X POST http://localhost:8000/api/v1/search/duckduckgo \
  -H "Content-Type: application/json" \
  -d '{"query": "python tutorial", "num_results": 15}'

# Website Scrape
curl -X POST http://localhost:8000/api/v1/website/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "extract_contacts": true}'

# Extract Contacts Only
curl "http://localhost:8000/api/v1/website/extract/contacts?url=https://example.com"

# Deep Scrape (Follow Links)
curl -X POST http://localhost:8000/api/v1/website/scrape/deep \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "max_depth": 2,
    "max_pages": 50
  }'
```

See [docs/USAGE.md](docs/USAGE.md) for comprehensive usage examples.

## ğŸ“š API Documentation

### Interactive Documentation

Once the application is running, access:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Main Endpoints

#### Search Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/search/google` | POST | Google search |
| `/api/v1/search/duckduckgo` | POST | DuckDuckGo search |
| `/api/v1/search/combined` | POST | Both engines simultaneously |
| `/api/v1/search/google/batch` | POST | Batch Google search |
| `/api/v1/search/duckduckgo/batch` | POST | Batch DuckDuckGo search |

#### Website Scraping Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/website/scrape` | POST | Scrape single website |
| `/api/v1/website/scrape/batch` | POST | Scrape multiple websites |
| `/api/v1/website/scrape/deep` | POST | Deep scrape (follow links) |
| `/api/v1/website/extract/contacts` | GET | Extract contacts only |
| `/api/v1/website/extract/content` | GET | Extract content only |
| `/api/v1/website/extract/metadata` | GET | Extract metadata only |

#### System Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check |
| `/status` | GET | Detailed system status |

## ğŸ³ Docker Deployment

### Using Docker Compose (Recommended)

```bash
# Start services (API + Redis)
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Start with Nginx (production)
docker-compose --profile production up -d
```

### Using Docker

```bash
# Build image
docker build -t web-scraper .

# Run container
docker run -d \
  -p 8000:8000 \
  -v $(pwd)/config:/app/config \
  -v $(pwd)/logs:/app/logs \
  --env-file .env \
  --name scraper-api \
  web-scraper

# View logs
docker logs -f scraper-api

# Stop container
docker stop scraper-api
```

### Production Deployment

```bash
# With Nginx reverse proxy
docker-compose --profile production up -d

# Scale API instances
docker-compose up -d --scale api=3
```

## âš¡ Performance

### Benchmarks

- **Search Scraping**: 60+ requests/minute
- **Website Scraping**: 30+ requests/minute
- **Concurrent Requests**: 50+ simultaneous
- **Success Rate**: 95%+ (with proxies and fallbacks)

### Performance Tuning

```env
# High-performance configuration
MAX_CONCURRENT_REQUESTS=100
API_WORKERS=8
MAX_SEARCH_REQUESTS_PER_MINUTE=120
MAX_WEBSITE_REQUESTS_PER_MINUTE=60
```

### Resource Requirements

| Setup | CPU | RAM | Throughput |
|-------|-----|-----|------------|
| Minimal | 1-2 cores | 2GB | 20 req/min |
| Standard | 4 cores | 8GB | 60 req/min |
| High | 8+ cores | 16GB | 120+ req/min |

## ğŸ—ï¸ Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FastAPI Application                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Search Scrapers â”‚        â”‚ Website Scraper  â”‚          â”‚
â”‚  â”‚  - Google        â”‚        â”‚  - Generic       â”‚          â”‚
â”‚  â”‚  - DuckDuckGo    â”‚        â”‚  - Deep Scrape   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Content Parsers  â”‚        â”‚ Request Handler  â”‚          â”‚
â”‚  â”‚  - HTML Parser   â”‚        â”‚  - aiohttp       â”‚          â”‚
â”‚  â”‚  - Contact Extr. â”‚        â”‚  - Playwright    â”‚          â”‚
â”‚  â”‚  - Metadata Extr.â”‚        â”‚  - Selenium      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Proxy Manager   â”‚  â”‚Rate Limiter  â”‚  â”‚Captcha Solverâ”‚ â”‚
â”‚  â”‚  - Rotation      â”‚  â”‚- Token Bucketâ”‚  â”‚- Audio/Image â”‚ â”‚
â”‚  â”‚  - Health Check  â”‚  â”‚- Redis/Memoryâ”‚  â”‚- Automation  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Redis       â”‚
                    â”‚ (Rate Limiting)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow

```
1. Client Request â†’ FastAPI Endpoint
2. Rate Limiting Check (Redis/Memory)
3. Request Handler â†’ Strategy Selection:
   â”œâ”€ Strategy 1: aiohttp + Proxy
   â”œâ”€ Strategy 2: aiohttp + Different Headers
   â”œâ”€ Strategy 3: Playwright (Browser)
   â””â”€ Strategy 4: Selenium (Browser)
4. Content Parser â†’ Extract & Categorize
5. Response â†’ Client
```

### Technologies

- **Framework**: FastAPI (async)
- **HTTP Client**: aiohttp (async)
- **HTML Parsing**: BeautifulSoup4, lxml
- **Browser Automation**: Playwright, Selenium
- **Rate Limiting**: Redis, in-memory fallback
- **Proxy Support**: HTTP, HTTPS, SOCKS5
- **Captcha**: Tesseract OCR, browser automation
- **Contact Extraction**: phonenumbers, email-validator
- **Structured Data**: extruct

## ğŸ”§ Advanced Features

### 1. Multiple Fallback Methods

Each request tries 2-3 different methods automatically:
- aiohttp with proxy rotation
- aiohttp with different headers/user agents
- Playwright browser automation
- Selenium browser automation

### 2. Proxy Management

- Automatic proxy rotation
- Health checking every 5 minutes
- Failure tracking and auto-disable
- Support for HTTP, HTTPS, SOCKS5

### 3. Rate Limiting

- Token bucket algorithm
- Per-endpoint limits
- Redis-backed (distributed)
- Automatic queueing

### 4. Content Extraction

- Intelligent paragraph detection
- Contact information extraction
- Structured data parsing (JSON-LD, Open Graph)
- Image and link extraction
- Table parsing

### 5. Error Handling

- Automatic retries with exponential backoff
- Multiple fallback strategies
- Detailed error logging
- Graceful degradation

## ğŸ“ Project Structure

```
web-scraping-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                    # API endpoints
â”‚   â”‚   â”œâ”€â”€ search_scraper.py   # Search API
â”‚   â”‚   â””â”€â”€ website_scraper.py  # Website API
â”‚   â”œâ”€â”€ core/                   # Core utilities
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py    # Proxy rotation
â”‚   â”‚   â”œâ”€â”€ request_handler.py  # HTTP requests
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py     # Rate limiting
â”‚   â”‚   â””â”€â”€ captcha_solver.py   # Captcha solving
â”‚   â”œâ”€â”€ scrapers/               # Scraper implementations
â”‚   â”‚   â”œâ”€â”€ google_scraper.py   # Google scraper
â”‚   â”‚   â”œâ”€â”€ duckduckgo_scraper.py # DuckDuckGo scraper
â”‚   â”‚   â””â”€â”€ generic_scraper.py  # Generic scraper
â”‚   â”œâ”€â”€ parsers/                # Content parsers
â”‚   â”‚   â”œâ”€â”€ content_parser.py   # HTML parser
â”‚   â”‚   â””â”€â”€ contact_extractor.py # Contact extractor
â”‚   â”œâ”€â”€ config/                 # Configuration
â”‚   â”‚   â””â”€â”€ settings.py         # Settings manager
â”‚   â”œâ”€â”€ utils/                  # Utilities
â”‚   â”‚   â”œâ”€â”€ user_agents.py      # User agent rotation
â”‚   â”‚   â””â”€â”€ helpers.py          # Helper functions
â”‚   â””â”€â”€ main.py                 # FastAPI application
â”œâ”€â”€ config/                     # Configuration files
â”‚   â”œâ”€â”€ config.yaml             # Main config
â”‚   â”œâ”€â”€ proxies.txt             # HTTP proxies
â”‚   â””â”€â”€ socks_proxies.txt       # SOCKS5 proxies
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ SETUP.md                # Setup guide
â”‚   â”œâ”€â”€ USAGE.md                # Usage guide
â”‚   â””â”€â”€ CONFIGURATION.md        # Config guide
â”œâ”€â”€ logs/                       # Log files
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Docker image
â”œâ”€â”€ docker-compose.yml          # Docker Compose
â”œâ”€â”€ .env.example                # Environment template
â””â”€â”€ README.md                   # This file
```

## ğŸ›¡ï¸ Security

### Best Practices

1. **Use environment variables** for sensitive data
2. **Enable authentication** for production (not included)
3. **Use HTTPS** with reverse proxy (Nginx)
4. **Restrict CORS origins** in production
5. **Keep proxies private** (don't commit to Git)
6. **Regular security updates** for dependencies

### Rate Limiting

Built-in rate limiting protects against abuse:
- 60 search requests/minute
- 30 website scrapes/minute
- Configurable per endpoint

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## âš ï¸ Legal Disclaimer

This tool is for educational and legitimate use only. Users are responsible for:

1. **Respecting robots.txt** and website terms of service
2. **Complying with applicable laws** in their jurisdiction
3. **Respecting rate limits** and not overloading servers
4. **Using appropriate authorization** when required
5. **Handling personal data** in compliance with privacy laws (GDPR, CCPA, etc.)

The authors assume no liability for misuse of this software.

## ğŸ™ Acknowledgments

- FastAPI for the excellent web framework
- Playwright and Selenium for browser automation
- BeautifulSoup4 for HTML parsing
- All open-source contributors

## ğŸ“ Support

- **Documentation**: See `/docs` folder
- **API Docs**: http://localhost:8000/docs
- **Issues**: GitHub Issues (if applicable)
- **Logs**: Check `logs/scraper.log`

## ğŸ—ºï¸ Roadmap

- [ ] API authentication and authorization
- [ ] Webhook support for async scraping
- [ ] Database integration for result storage
- [ ] Advanced scheduling and cron jobs
- [ ] More search engines (Bing, Yahoo, etc.)
- [ ] PDF and document parsing
- [ ] Machine learning for content classification
- [ ] Distributed scraping cluster support

---

**Built with â¤ï¸ using Python and FastAPI**
